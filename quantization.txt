~ Quantization 코드기준 접근 방향 ~

<ONNX 기준 Post Training Quantization 진행>
1. pretrained model을 onnx 형태로 변환

2. preprocess 진행 (ex. python -m onnxruntime.quantization.preprocess --input test.onnx --output preprocess_test.onnx)

3. preprocess로 저정한 onnx 파일을 가지고 quantization 진행
   CNN은 static quantization으로 진행해야함
   CNN을 제외한 다른 것은 dynamic quantization으로 진행해야함 
   CNN과 다른 연산이 혼재된 경우에는 아래와 같이 dynamic으로 Conv 연산이 매우 작을경우에는 아래와 같이 Conv 연산만 제외하고 선언해서 사용가능
     quantize_dynamic('preprocess_test.onnx', 'test_quantized.onnx', weight_type=QuantType.QInt8,op_types_to_quantize=['MatMul', 'Attention', 'LSTM', 'Gather', 'Transpose', 'EmbedLayerNormalization'] )
   CNN이 포함되어야 하면 static 진행해야하며 이때는 CalibrationDataReader가 필요하게됨 ( static 이랑 dynamic 차이 보고 확인해야할듯? )
   static quantization ==> 주로 CNN에서 사용됨

<TENSORRT 기준 Post Training Quantization 진행>
1. ONNX로 저장한 파일 load 이후 tensorRT engine으로 빌드진행 (빌드진행시에 16bit quantization 하려면 config.set_flag(trt.BuilderFlag.FP16) 써줌)

2. inferece 진행시에는 저장한 engine load 이후에 input, output에 대하여 host memory(cpu), device memory(gpu)로 처리하기 위한 binding 처리 진행

3. 이후에 binding한 것을 호출하여 host to device (함수 cuda.memcpy_htod)로 input data gpu로 넘기고 inference 진행
